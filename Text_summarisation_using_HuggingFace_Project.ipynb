{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvE0a69Qh+IF6EFh4UNd/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravinnd3/Generative-AI-Full-Course/blob/main/Text_summarisation_using_HuggingFace_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "g2aJGsski_g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC5nGFFCixCS"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[sentencepiece] datasets sacrebleu rouge_score py7zr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate evaluate"
      ],
      "metadata": {
        "id": "qpzP8_ygjFhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk\n",
        "# from evaluate import load_metric\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "PkjwJKVzkJXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "atxodrYgk-HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "0_q3Pm9KnHZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "amOMwQJMns9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum = load_dataset(\"knkarthick/samsum\")\n",
        "dataset_samsum\n",
        "#"
      ],
      "metadata": {
        "id": "XmVkgOIRoJQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(\n",
        "        example_batch[\"dialogue\"], max_length=1024, truncation=True\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(\n",
        "            example_batch[\"summary\"], max_length=128, truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "        \"input_ids\": input_encodings[\"input_ids\"],\n",
        "        \"attention_mask\": input_encodings[\"attention_mask\"],\n",
        "        \"labels\": target_encodings[\"input_ids\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "yR65tVS7oMxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "id": "10wAQWfGqV9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt['train']"
      ],
      "metadata": {
        "id": "nyTnsd9QqY5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt['train']['input_ids'][0]\n"
      ],
      "metadata": {
        "id": "9Yc48A2drYsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n"
      ],
      "metadata": {
        "id": "ITczqAUard0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir=\"pegasus-samsum\", num_train_epochs=1, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    do_eval=True, # Ensure evaluation is enabled\n",
        "    # evaluation_strategy=\"steps\", # This argument caused the TypeError\n",
        "    # eval_steps=500,              # This argument is tied to evaluation_strategy\n",
        "    save_steps=1000000, # Changed from 1e6 (float) to 1000000 (int)\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ],
      "metadata": {
        "id": "p-F1P4whsAsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
        "                  processing_class=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ],
      "metadata": {
        "id": "N5tEIk5KsLyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0cqgbokEswWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_meric_on_train_eval_data(dataset, metric, model, tokenizer,\n",
        "                                       batch_size=16, device=device,\n",
        "                                       column_text=\"article\",\n",
        "                                       column_summary=\"highlights\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
        "                           padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                                   attention_mask=inputs[\"attention_masks\"].to(device),\n",
        "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        #decode the generated texts,\n",
        "        #replace the token, and add the decoded texts with the refrence to the metric\n",
        "\n",
        "        decode_summries = [d.replace(\"<pad>\", \" \") for d in tokenizer.batch_decode(summaries, skip_special_tokens=True)]\n",
        "\n",
        "        decode_summries = [d.replace(\"?\", \" \") for d in decode_summries]\n",
        "\n",
        "        metric.add_batch(predictions=decode_summries, references=target_batch)\n",
        "\n",
        "    score = metric.compute()\n",
        "    return score"
      ],
      "metadata": {
        "id": "NkzCGeTcs6eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = [\"rouge1\",\"rouge2\",\"rougeL\",\"rougeLsum\"]\n",
        "rouge_metric = load_metric('rouge')"
      ],
      "metadata": {
        "id": "VCGNltXqvfcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}